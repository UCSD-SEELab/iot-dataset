{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-volunteer",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2021/MG.csv')\n",
    "print(df.shape)\n",
    "print(df.dropna().shape)\n",
    "df.head()\n",
    "\n",
    "feature = 'Ua'\n",
    "\n",
    "#df=pd.read_csv('../../ARIMA-Temperature_Forecasting/MaunaLoaDailyTemps.csv',index_col='DATE',parse_dates=True)\n",
    "#df=df.dropna()\n",
    "#print('Shape of data',df.shape)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[feature].dropna().plot(figsize=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-explanation",
   "metadata": {},
   "source": [
    "## Check for stationarity (p-value should be less than 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(dataset):\n",
    "  dftest = adfuller(dataset, autolag = 'AIC')\n",
    "  print(\"1. ADF : \",dftest[0])\n",
    "  print(\"2. P-Value : \", dftest[1])\n",
    "  print(\"3. Num Of Lags : \", dftest[2])\n",
    "  print(\"4. Num Of Observations Used For ADF Regression and Critical Values Calculation :\", dftest[3])\n",
    "  print(\"5. Critical Values :\")\n",
    "  for key, val in dftest[4].items():\n",
    "      print(\"\\t\",key, \": \", val)\n",
    "        \n",
    "adf_test(df[feature].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-engineering",
   "metadata": {},
   "source": [
    "## Figure out order for ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "\n",
    "stepwise_fit = auto_arima(df[feature].dropna(), \n",
    "                          suppress_warnings=True)           \n",
    "\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424f094",
   "metadata": {},
   "source": [
    "## Define order based on the output of auto_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stepwise_fit.order)\n",
    "order = stepwise_fit.order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-infection",
   "metadata": {},
   "source": [
    "## Split Data into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropna = df.dropna()\n",
    "print(df_dropna.shape)\n",
    "test_samples_num = int(0.04 * df_dropna.shape[0])\n",
    "train = df_dropna.iloc[:-test_samples_num]\n",
    "test = df_dropna.iloc[-test_samples_num:]\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-racing",
   "metadata": {},
   "source": [
    "## Train the ARIMA model and perform rolling predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "history = list(train[feature])\n",
    "predictions = list()\n",
    "print(len(test))\n",
    "for t in test[feature].index:\n",
    "    model = ARIMA(history, order=order)\n",
    "    model_fit = model.fit()\n",
    "    output = model_fit.forecast()\n",
    "    predictions.append(output[0])\n",
    "    #history.append(output[0])\n",
    "    history.append(test[feature][t])\n",
    "    print('predicted=%f, expected=%f' % (output[0], test[feature][t]))\n",
    "    \n",
    "# evaluate forecasts\n",
    "rmse = sqrt(mean_squared_error(test[feature], predictions))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61529e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot forecasts against actual outcomes\n",
    "plt.plot(np.arange(len(test)), test[feature])\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0156673c",
   "metadata": {},
   "source": [
    "## Test residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c558f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residual errors\n",
    "model_fit.plot_diagnostics(figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a72bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpwren",
   "language": "python",
   "name": "hpwren"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
